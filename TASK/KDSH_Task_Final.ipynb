{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0b46ee-8535-4d7f-884c-ad7e58eee692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'tabulator': 'https://cdn.jsdelivr.net/npm/tabulator-tables@6.3.0/dist/js/tabulator.min', 'moment': 'https://cdn.jsdelivr.net/npm/luxon/build/global/luxon.min'}, 'shim': {}});\n      require([\"tabulator\"], function(Tabulator) {\n        window.Tabulator = Tabulator\n        on_load()\n      })\n      require([\"moment\"], function(moment) {\n        window.moment = moment\n        on_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.Tabulator !== undefined) && (!(window.Tabulator instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/js/tabulator.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    if (((window.moment !== undefined) && (!(window.moment instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/luxon/build/global/luxon.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/js/tabulator.min.js\", \"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/luxon/build/global/luxon.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.holoviz.org/panel/1.5.5/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [\"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/css/tabulator_simple.min.css?v=1.5.5\"];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86'>\n",
       "  <div id=\"b6d6862f-5ec2-4fb2-8c2c-9c954c68deac\" data-root-id=\"5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"1d5ec5bc-9182-49da-a01a-0aa9618b0bea\":{\"version\":\"3.6.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"9546989a-4b06-442a-98df-d66b52ca267d\",\"attributes\":{\"plot_id\":\"5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86\",\"comm_id\":\"569eb83c396142c3960cdda447f4607c\",\"client_comm_id\":\"806cb851076a42f19ded101bfe3984fb\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"1d5ec5bc-9182-49da-a01a-0aa9618b0bea\",\"roots\":{\"5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86\":\"b6d6862f-5ec2-4fb2-8c2c-9c954c68deac\"},\"root_ids\":[\"5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.Tabulator !== undefined) && ( root.Tabulator !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "5a55a1d7-44dd-4ff7-9a1d-e408d18f7b86"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from PyPDF2 import PdfReader\n",
    "import pathway as pw\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "#Suppress logs from specific libraries\n",
    "logging.getLogger('pathway_engine').setLevel(logging.WARNING)\n",
    "logging.getLogger('aiohttp.access').setLevel(logging.WARNING)\n",
    "logging.getLogger('root').setLevel(logging.WARNING)  # General log level for all root logs\n",
    "#Suppress logs from the 'requests' library or any other libraries similarly\n",
    "logging.getLogger('requests').setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger('pathway_engine.connectors.monitoring')\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "import json\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "GEMINI_API_KEY=config[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e27ba-8ce6-49e8-9e40-24998fe3dc64",
   "metadata": {},
   "source": [
    "### Importing Required Libraries and Initializing NLP Tools\n",
    "\n",
    "This step involves the initialization of several important libraries and tools that will be used for text preprocessing. Here's a breakdown of each part of the code:\n",
    "\n",
    "1. **Downloading NLTK Stopwords**:\n",
    "   - `nltk.download('stopwords')`: This command downloads a list of common stopwords (e.g., \"the\", \"is\", \"in\", etc.) that are typically removed during text preprocessing to improve the quality of the text analysis.\n",
    "\n",
    "2. **Defining Excluded Punctuation**:\n",
    "   - `exclude = string.punctuation`: This sets a variable `exclude` that contains all the punctuation marks. This will be useful when we need to remove punctuation from the text during preprocessing.\n",
    "\n",
    "3. **Initializing Stemming and Lemmatization Tools**:\n",
    "   - `lemmatizer = WordNetLemmatizer()`: This initializes the **WordNet Lemmatizer** from NLTK, which will be used for lemmatization. Lemmatization involves reducing a word to its base or dictionary form (e.g., \"better\" becomes \"good\").\n",
    "\n",
    "#### Purpose:\n",
    "- **Text Preprocessing**: These tools are essential for text preprocessing tasks, such as removing stopwords, punctuation, and reducing words to their base forms (lemmatization).\n",
    "- **Efficiency in NLP Tasks**: By removing unnecessary elements (like stopwords and punctuation), we ensure that the text is cleaner and more suitable for machine learning or further NLP tasks, such as text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc64a971-6beb-4751-9cfb-985136ec9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malyadippal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "exclude=string.punctuation\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee306b-25eb-4464-ac13-caa7a778f1d2",
   "metadata": {},
   "source": [
    "### 1. `text_extractor_from_pdf(pdf_path)`\n",
    "\n",
    "This function extracts the text from all the pages of a given PDF file.\n",
    "\n",
    "- **Input:** Path to a PDF file.\n",
    "- **Process:** It uses `PdfReader` from the `PyPDF2` library to read the content of the PDF. Then, it iterates over each page and extracts the text.\n",
    "- **Output:** A string containing the combined text from all pages of the PDF.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `lemmatize_words(text)`\n",
    "\n",
    "This function lemmatizes the words in the given text.\n",
    "\n",
    "- **Input:** A string of text.\n",
    "- **Process:** It splits the text into individual words and applies the `lemmatizer` to reduce each word to its base form (lemma).\n",
    "- **Output:** A string of lemmatized words joined together.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `remove_stopwords(text)`\n",
    "\n",
    "This function removes common stopwords from the input text.\n",
    "\n",
    "- **Input:** A string of text.\n",
    "- **Process:** The function iterates over the words in the text and removes those present in the English stopwords list from the `stopwords` library.\n",
    "- **Output:** A string where the stopwords are removed, leaving only meaningful words.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `preprocessed_text(text)`\n",
    "\n",
    "This function preprocesses the input text by applying several text cleaning and normalization techniques.\n",
    "\n",
    "- **Input:** A string of text.\n",
    "- **Process:**\n",
    "  1. Converts the text to lowercase.\n",
    "  2. Removes any HTML tags using a regular expression.\n",
    "  3. Removes URLs using a regular expression.\n",
    "  4. Strips punctuations and symbols using `str.translate`.\n",
    "  5. Removes stopwords by calling `remove_stopwords`.\n",
    "  6. Lemmatizes the words by calling `lemmatize_words`.\n",
    "- **Output:** A cleaned and preprocessed version of the input text, ready for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc795e91-ee2a-4e1a-b056-2f3320b14cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "def text_extractor_from_pdf(pdf_path):\n",
    "    reader=PdfReader(pdf_path)\n",
    "    text=\"\"\n",
    "    for page in reader.pages:\n",
    "        text+=page.extract_text()\n",
    "    return text\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "def preprocessed_text(text):\n",
    "    text=text.lower() # Lowercase Conversion\n",
    "    text=re.sub(r'<.*?>','',text) #Removing html tags, though it might not be present in this pdfs\n",
    "    text=re.sub(r'https?://\\S+|www\\.\\S+','',text) # Removing URLs\n",
    "    text=text.translate(str.maketrans('','',exclude)) #Removing punctuations & symbols\n",
    "    text=remove_stopwords(text) #Removing stop words\n",
    "    text=lemmatize_words(text) #Lemmatizing the words in the text\n",
    "    return text\n",
    "#-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc5cc0-c041-4ebb-9e35-44d0bbb265ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef2cf1-8b3b-4f9e-8fa8-8d02b58b5e81",
   "metadata": {},
   "source": [
    "###                                                    `Task 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2236067-d64d-4613-9426-4f6c91fe009a",
   "metadata": {},
   "source": [
    "### 1. `non_publishable_folder` and `non_publishable_texts`\n",
    "\n",
    "This block processes PDFs in the \"Reference/Non-Publishable\" folder and extracts the preprocessed text from each file.\n",
    "\n",
    "- **Process:**\n",
    "  1. Defines the folder path (`non_publishable_folder`).\n",
    "  2. Loops through each file in the folder and checks if the file has a `.pdf` extension.\n",
    "  3. For each PDF file, it extracts the text using `text_extractor_from_pdf` and preprocesses the text using `preprocessed_text`.\n",
    "  4. Appends the processed text to the `non_publishable_texts` list.\n",
    "- **Output:** A list of preprocessed texts from the non-publishable PDF files.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `publishable_folder` and `publishable_texts`\n",
    "\n",
    "This block processes PDFs in multiple folders related to publishable documents (CVPR, EMNLP, KDD, NeurIPS, TMLR) and extracts the preprocessed text from each file.\n",
    "\n",
    "- **Process:**\n",
    "  1. Defines a list of folder paths (`publishable_folder`), each representing a specific publication venue.\n",
    "  2. Loops through each folder, checking for `.pdf` files.\n",
    "  3. For each PDF file, it extracts and preprocesses the text as done in the previous block.\n",
    "  4. Appends the processed text to the `publishable_texts` list.\n",
    "- **Output:** A list of preprocessed texts from the publishable PDF files across all the specified folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6b7d77-83c1-4690-be00-b6b67765715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "non_publishable_folder='Reference/Non-Publishable/'\n",
    "non_publishable_texts=[]\n",
    "for file in os.listdir(non_publishable_folder):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path=os.path.join(non_publishable_folder,file)\n",
    "        non_publishable_texts.append(preprocessed_text(text_extractor_from_pdf(pdf_path)))\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "publishable_folder=['Reference/Publishable/CVPR/','Reference/Publishable/EMNLP/','Reference/Publishable/KDD/','Reference/Publishable/NeurIPS/','Reference/Publishable/TMLR/']\n",
    "publishable_texts=[]\n",
    "for folder in publishable_folder:\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path=os.path.join(folder,file)\n",
    "            publishable_texts.append(preprocessed_text(text_extractor_from_pdf(pdf_path)))\n",
    "#-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29e8d3-2162-468a-af90-96b42d87dff0",
   "metadata": {},
   "source": [
    "### 3. Creating DataFrame from Texts\n",
    "\n",
    "In this block, a `pandas` DataFrame is created from the preprocessed texts (both publishable and non-publishable) and their corresponding labels.\n",
    "\n",
    "- **Process:**\n",
    "  1. A dictionary `data` is created, where:\n",
    "     - The `\"Text\"` key contains a combined list of `publishable_texts` and `non_publishable_texts`.\n",
    "     - The `\"Publishable\"` key contains labels (1 for publishable, 0 for non-publishable) with the same length as the respective text lists.\n",
    "  2. A DataFrame `df` is created using the `data` dictionary.\n",
    "  3. The DataFrame is shuffled using `.sample(frac=1)` and reset with a new index (`reset_index(drop=True)`).\n",
    "- **Output:** A shuffled DataFrame (`df`) with columns `\"Text\"` and `\"Publishable\"`, where each text is labeled as either publishable (1) or non-publishable (0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09483e0-3db5-4a65-9dd6-c32d2273b2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Publishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>detailed action identification baseball game r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advanced technique contextually interpreting n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>denoising diffusion probabilistic model jonath...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advancement 3d food modeling review metafood c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch imperative style highperformance deep ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Publishable\n",
       "0  detailed action identification baseball game r...            1\n",
       "1  advanced technique contextually interpreting n...            1\n",
       "2  denoising diffusion probabilistic model jonath...            1\n",
       "3  advancement 3d food modeling review metafood c...            1\n",
       "4  pytorch imperative style highperformance deep ...            1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data={\n",
    "    \"Text\":publishable_texts+non_publishable_texts,\n",
    "    \"Publishable\":[1]*len(publishable_texts)+[0]*len(non_publishable_texts)\n",
    "}\n",
    "df=pd.DataFrame(data)\n",
    "df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019bbd9-eb1e-4f74-b0d9-cd89e3739c25",
   "metadata": {},
   "source": [
    "### 4. Data Balancing and Splitting into Features and Labels\n",
    "\n",
    "In this block, the DataFrame is balanced by downsampling the publishable texts to match the number of non-publishable texts, and then it is split into features and labels.\n",
    "\n",
    "- **Process:**\n",
    "  1. **Splitting the DataFrame:** \n",
    "     - `df_publishable`: Contains only publishable texts (`Publishable == 1`).\n",
    "     - `df_non_publishable`: Contains only non-publishable texts (`Publishable == 0`).\n",
    "  2. **Downsampling:**\n",
    "     - `df_publishable_downsampled`: The publishable texts are downsampled to match the number of non-publishable texts using `.sample()`.\n",
    "  3. **Concatenating and Shuffling:**\n",
    "     - `df_balanced`: The downsampled publishable texts and non-publishable texts are concatenated using `pd.concat()`. The result is shuffled using `.sample(frac=1)` and reset with a new index (`reset_index(drop=True)`).\n",
    "  4. **Feature and Label Split:**\n",
    "     - `X`: The feature matrix, containing only the `\"Text\"` column.\n",
    "     - `y`: The label vector, which contains the `\"Publishable\"` column converted to a NumPy array of integers.\n",
    "\n",
    "- **Output:** \n",
    "  - `df_balanced`: A balanced DataFrame of publishable and non-publishable texts.\n",
    "  - `X`: The features (texts).\n",
    "  - `y`: The corresponding labels (1 for publishable, 0 for non-publishable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6dc4fec-0506-4e43-9225-bddf22e8a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publishable=df[df['Publishable']==1]\n",
    "df_non_publishable=df[df['Publishable']==0]\n",
    "#------------------------------------------------------------------------------------\n",
    "df_publishable_downsampled=df_publishable.sample(df_non_publishable.shape[0],random_state=42)\n",
    "#------------------------------------------------------------------------------------\n",
    "df_balanced=pd.concat([df_publishable_downsampled,df_non_publishable])\n",
    "df_balanced=df_balanced.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "#------------------------------------------------------------------------------------\n",
    "X=df_balanced.iloc[:,0:1]\n",
    "y=df_balanced['Publishable'].to_numpy(dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1cec9b-7f26-4645-99df-dd2869539b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Publishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aidriven personalization online education plat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detecting medication usage parkinson’s disease...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synergistic convergence photosynthetic pathway...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>importance written explanation aggregating cro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analyzing realtime group coordination augmente...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>detailed action identification baseball game r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transdimensional property graphite relation ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proceeding 2023 conference empirical method na...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>generalization relu network via restricted iso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deciphering enigmatic property metal critical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Publishable\n",
       "0  aidriven personalization online education plat...            0\n",
       "1  detecting medication usage parkinson’s disease...            1\n",
       "2  synergistic convergence photosynthetic pathway...            0\n",
       "3  importance written explanation aggregating cro...            1\n",
       "4  analyzing realtime group coordination augmente...            0\n",
       "5  detailed action identification baseball game r...            1\n",
       "6  transdimensional property graphite relation ch...            0\n",
       "7  proceeding 2023 conference empirical method na...            1\n",
       "8  generalization relu network via restricted iso...            1\n",
       "9  deciphering enigmatic property metal critical ...            0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c12524-22c1-4249-9e51-f75d4beb66ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aidriven personalization online education plat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detecting medication usage parkinson’s disease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synergistic convergence photosynthetic pathway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>importance written explanation aggregating cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analyzing realtime group coordination augmente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>detailed action identification baseball game r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transdimensional property graphite relation ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proceeding 2023 conference empirical method na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>generalization relu network via restricted iso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deciphering enigmatic property metal critical ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  aidriven personalization online education plat...\n",
       "1  detecting medication usage parkinson’s disease...\n",
       "2  synergistic convergence photosynthetic pathway...\n",
       "3  importance written explanation aggregating cro...\n",
       "4  analyzing realtime group coordination augmente...\n",
       "5  detailed action identification baseball game r...\n",
       "6  transdimensional property graphite relation ch...\n",
       "7  proceeding 2023 conference empirical method na...\n",
       "8  generalization relu network via restricted iso...\n",
       "9  deciphering enigmatic property metal critical ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "195f8026-a3a5-4ad8-8892-791d27caff6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acff34b-384c-41d1-b294-ab9fabb0f16b",
   "metadata": {},
   "source": [
    "### 5. Splitting Texts for Model Accuracy Check\n",
    "\n",
    "In this block, we extract a subset of the **publishable texts** that were **not** included in the downsampled data. This subset will later be used to check the accuracy of the model after training.\n",
    "\n",
    "- **Process:**\n",
    "  - `X_temp`: A DataFrame containing publishable texts that were **excluded** from the downsampling process. This is achieved by:\n",
    "    1. Selecting rows where the `\"Publishable\"` column is `1` (i.e., publishable texts).\n",
    "    2. Using the `~df['Text'].isin(df_publishable_downsampled['Text'])` condition to filter out texts that are already present in the downsampled publishable texts (`df_publishable_downsampled`).\n",
    "  \n",
    "- **Purpose:**\n",
    "  - `X_temp` will be used to evaluate the **accuracy of the model** once it is trained, ensuring that the test data remains unseen during the training phase.\n",
    "\n",
    "- **Output:**\n",
    "  - `X_temp`: A DataFrame of the publishable texts that are not part of the training dataset, reserved for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b6563cc-6072-4787-84f8-06d11e37e04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Publishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advanced technique contextually interpreting n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>denoising diffusion probabilistic model jonath...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advancement 3d food modeling review metafood c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch imperative style highperformance deep ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>examining convergence denoising diffusion prob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>proceeding 2023 conference empirical method na...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>safe predictor inputoutput specification enfor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>addressing minmax challenge nonconvexnonconcav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>addressing popularity bias popularityconscious...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Publishable\n",
       "1   advanced technique contextually interpreting n...            1\n",
       "2   denoising diffusion probabilistic model jonath...            1\n",
       "3   advancement 3d food modeling review metafood c...            1\n",
       "4   pytorch imperative style highperformance deep ...            1\n",
       "7   examining convergence denoising diffusion prob...            1\n",
       "10  proceeding 2023 conference empirical method na...            1\n",
       "11  safe predictor inputoutput specification enfor...            1\n",
       "14  addressing minmax challenge nonconvexnonconcav...            1\n",
       "18  addressing popularity bias popularityconscious...            1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_temp = df[(df['Publishable'] == 1) & (~df['Text'].isin(df_publishable_downsampled['Text']))]\n",
    "X_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924379a2-d41d-4a2a-bd83-e43ef0701936",
   "metadata": {},
   "source": [
    "### 6. Splitting Data into Training and Testing Sets\n",
    "\n",
    "In this step, the data is split into training and testing sets using the `train_test_split` function from scikit-learn.\n",
    "\n",
    "- **Process:**\n",
    "  - `X_train`, `X_test`: These variables represent the features (text data) for training and testing, respectively.\n",
    "  - `y_train`, `y_test`: These variables represent the target labels (publishable or non-publishable) for training and testing.\n",
    "  - The split is performed with a fixed `random_state=1` to ensure reproducibility.\n",
    "\n",
    "- **Purpose:**\n",
    "  - The data is randomly divided so that the model can be trained on one subset (`X_train`, `y_train`) and tested on an unseen subset (`X_test`, `y_test`).\n",
    "  \n",
    "- **Output:**\n",
    "  - The dataset is split into two sets for training and evaluation, allowing the model to learn from the training set and be evaluated on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0f448c-1b33-44d8-9abe-5fa83ec15cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75289383-d3ed-498d-afc0-ffd4af1a3628",
   "metadata": {},
   "source": [
    "### 7. Text Feature Extraction: Bag of Words (BOW) and TF-IDF\n",
    "\n",
    "In this step, two common text feature extraction techniques—**Bag of Words (BOW)** and **Term Frequency-Inverse Document Frequency (TF-IDF)**—are applied to the text data for training and testing.\n",
    "\n",
    "#### 7.1 Bag of Words (BOW)\n",
    "- **CountVectorizer** is used to convert the text data into a matrix of token counts.\n",
    "  - `X_train_bow`: The training text data is transformed using `fit_transform` into an array of token counts (BOW representation).\n",
    "  - `X_test_bow`: The test text data is transformed using the `transform` method (no fitting here, only transforming based on the training data).\n",
    "  \n",
    "  **BOW** treats the text as an unordered collection of words and represents it as a sparse matrix, where each word is represented by a count.\n",
    "\n",
    "#### 7.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- **TfidfVectorizer** is used to convert the text data into a matrix that reflects the importance of words.\n",
    "  - `X_train_tfidf`: The training text data is transformed using `fit_transform` into an array where each word is weighted by its term frequency and inverse document frequency.\n",
    "  - `X_test_tfidf`: The test text data is transformed using `transform` based on the training data's learned vocabulary and IDF values.\n",
    "  \n",
    "  **TF-IDF** assigns higher importance to words that appear frequently in a specific document but less often across all documents.\n",
    "\n",
    "#### Purpose:\n",
    "- These transformations convert raw text data into numerical format, making it suitable for machine learning algorithms.\n",
    "- The BOW and TF-IDF models provide different perspectives on the text's content, with BOW focusing on word counts and TF-IDF adjusting for common terms across documents.\n",
    "\n",
    "#### Output:\n",
    "- `X_train_bow`, `X_test_bow`: Represent the training and testing data using BOW.\n",
    "- `X_train_tfidf`, `X_test_tfidf`: Represent the training and testing data using TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74fb126a-abf2-460e-af3c-65aa2ae2b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "#Applying BOW\n",
    "cv=CountVectorizer()\n",
    "X_train_bow=cv.fit_transform(X_train['Text']).toarray()\n",
    "X_test_bow=cv.transform(X_test['Text']).toarray()\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#Applying Tf_Idf\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['Text']).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test['Text']).toarray()\n",
    "#-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d384b57-b5b8-43ec-9861-96e388ca72cd",
   "metadata": {},
   "source": [
    "### 8. Model Evaluation: Gaussian Naive Bayes (GaussianNB) with BOW and TF-IDF\n",
    "\n",
    "In this section, we apply **Gaussian Naive Bayes (GaussianNB)** using both **Bag of Words (BOW)** and **TF-IDF** feature representations and evaluate the model performance.\n",
    "\n",
    "#### 8.1 Gaussian Naive Bayes (BOW)\n",
    "- **Model Training**: A **Gaussian Naive Bayes (GaussianNB)** model is created and trained using the BOW features (`X_train_bow`) and the corresponding target labels (`y_train`).\n",
    "- **Prediction**: The model makes predictions on the test set (`X_test_bow`), and the accuracy score is calculated.\n",
    "  - `accuracy_score`: Measures the percentage of correct predictions.\n",
    "- **F1 Score**: The **F1 Score** is calculated using the `f1_score` function with weighted average, which considers both precision and recall.\n",
    "  - `f1_score`: Provides a more balanced metric, especially useful for imbalanced datasets.\n",
    "- **Cross-Validation**: **5-fold cross-validation** is applied to assess the model's performance across different splits of the training data.\n",
    "  - `cross_val_score`: Provides the average accuracy score from the cross-validation process.\n",
    "\n",
    "#### 8.2 Gaussian Naive Bayes (TF-IDF)\n",
    "- Similar steps are repeated for the **TF-IDF** features (`X_train_tfidf`), with the same evaluation metrics applied:\n",
    "  - **Accuracy**: Assesses the model's performance in terms of correct predictions.\n",
    "  - **F1 Score**: Evaluates the harmonic mean of precision and recall.\n",
    "  - **Cross-Validation Accuracy**: Measures how well the model generalizes by evaluating it on different training-validation splits.\n",
    "\n",
    "\n",
    "#### Results:\n",
    "- **Accuracy Score using BOW:** 1.0\n",
    "- **F1 Score:** 1.0\n",
    "- **Cross-Validation Accuracy using BOW:** 0.6\n",
    "\n",
    "- **Accuracy Score using TF-IDF:** 1.0\n",
    "- **F1 Score:** 1.0\n",
    "- **Cross-Validation Accuracy using TF-IDF:** 0.6\n",
    "\n",
    "#### Possible Reasons for the Results:\n",
    "- **Accuracy Score and F1 Score of 1.0:** The perfect accuracy and F1 scores suggest that the model is performing exceptionally well on the test data. However, this might indicate **overfitting**, where the model has memorized the training data and performs well on data similar to it, but might not generalize well on unseen data.\n",
    "\n",
    "- **Cross-Validation Accuracy of 0.6:** The discrepancy between the high accuracy scores and the lower cross-validation score suggests that while the model performs well on certain data splits, it might struggle on others. This indicates that the model is likely **overfitting** to the training data, and its performance is less consistent across different subsets of the data.\n",
    "\n",
    "In summary, the perfect scores on the test data (accuracy and F1 score) suggest that the model might not be generalizing well, and the cross-validation score of 0.6 confirms that the model's performance is not as reliable across all data splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55994cec-b228-4cdd-a8c9-c07596795395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score using bow: 1.0\n",
      "F1 Score: 1.0\n",
      "Cross-Validation Accuracy using bow: 0.8\n",
      "\n",
      "Accuracy Score using tfidf: 1.0\n",
      "F1 Score: 1.0\n",
      "Cross-Validation Accuracy using tfidf: 0.6\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "#GaussiaNB using BOW\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(X_train_bow,y_train)\n",
    "y_pred=gnb.predict(X_test_bow)\n",
    "print(\"Accuracy Score using bow:\",accuracy_score(y_test,y_pred))\n",
    "f1=f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score:\",f1)\n",
    "scores = cross_val_score(gnb, X_train_bow, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy using bow:\", scores.mean())\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#GaussiaNB using TfIdf\n",
    "gnbtf=GaussianNB()\n",
    "gnbtf.fit(X_train_tfidf,y_train)\n",
    "y_pred_tf=gnbtf.predict(X_test_tfidf)\n",
    "print(\"\\nAccuracy Score using tfidf:\",accuracy_score(y_test,y_pred_tf))\n",
    "f1=f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 Score:\",f1)\n",
    "scores = cross_val_score(gnb, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy using tfidf:\", scores.mean())\n",
    "#-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd2a95-d02c-4966-bd9a-c8aad5cd3377",
   "metadata": {},
   "source": [
    "### Analyzing Top Features Influencing Predictions\n",
    "\n",
    "In this section, we analyze the **top features** that significantly influence the model's predictions. This step helps us understand the model's decision-making process.\n",
    "\n",
    "#### Purpose of Viewing Top Features:\n",
    "- **Interpretability**: By identifying the top features, we can interpret which words or terms the **Gaussian Naive Bayes (GNB)** model relies on to classify text as **publishable** or **non-publishable**.\n",
    "- **Feature Importance**: These features are the most important ones in distinguishing between the classes, and understanding them helps us grasp which factors contribute to the model's predictions.\n",
    "\n",
    "This step is important for **model interpretability**, allowing us to see which words are most indicative of whether a document is publishable or not. It provides valuable insights into the model's behavior and the underlying patterns it is using to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33ca8ba-179f-4535-9015-c7e77540f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features influencing predictions:\n",
      "['question' 'feature' 'parameterefficient' 'network' 'parameter' 'large'\n",
      " 'tuning' 'method' 'language' 'model']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top features influencing predictions:\")\n",
    "feature_names = np.array(cv.get_feature_names_out())\n",
    "log_probs = gnb.theta_\n",
    "top_features = np.argsort(log_probs[1] - log_probs[0])[-10:]\n",
    "print(feature_names[top_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75515b8-b328-4990-95f4-26dfcdfe1457",
   "metadata": {},
   "source": [
    "### 5th Step Continuation: Model Validation on Remaining Publishable Texts\n",
    "\n",
    "In this step, we evaluate the model's performance on the texts that were originally classified as \"Publishable\" but were not included in the downsampled dataset used for training. This is done to check how well the model generalizes to unseen data.\n",
    "\n",
    "The code transforms the text in `X_temp` into a bag-of-words representation (`X_temp_bow`), applies the trained Gaussian Naive Bayes (GNB) model to predict the labels, and then compares the predictions (`y_temp`) with the actual labels in `X_temp['Publishable']` using accuracy as the metric.\n",
    "\n",
    "This step ensures that the model's performance is not biased by the training data and provides an estimate of how well it can predict on new, unseen \"Publishable\" texts.\n",
    "\n",
    "#### Possible Reason for High Accuracy:\n",
    "The accuracy score is coming out as 1, it might indicate that the model is overfitting to the training data, or that the validation set (`X_temp`) contains texts very similar to the ones in the training set. This can lead to the model performing exceptionally well on the validation data, which may not reflect its true performance on entirely unseen data. Additionally, if the \"Publishable\" texts are very distinct or the dataset is small, the model might be able to easily classify the texts with high accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7e3ca9-e24e-4f72-8e44-b1d99dda43ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_temp_bow=cv.transform(X_temp['Text']).toarray()\n",
    "y_temp=gnb.predict(X_temp_bow)\n",
    "accuracy_score(X_temp['Publishable'].to_numpy(dtype=np.int32),y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778c79a-1645-4a76-8c47-1654a830940c",
   "metadata": {},
   "source": [
    "### 9. Model Evaluation Using Logistic Regression (BOW and TF-IDF)\n",
    "\n",
    "In this step, **Logistic Regression (LR)** is applied to the dataset using both **Bag of Words (BOW)** and **TF-IDF** vectorization techniques. Similar to the previous step, the model is evaluated on the test data (`X_test_bow` and `X_test_tfidf`) using accuracy, F1 score, and cross-validation.\n",
    "\n",
    "The code trains the **Logistic Regression** model on the BOW and TF-IDF representations of the training data and then evaluates its performance.\n",
    "\n",
    "#### Results:\n",
    "- **Accuracy Score using BOW:** 1.0\n",
    "- **F1 Score:** 1.0\n",
    "- **Cross-Validation Accuracy using BOW:** 0.9\n",
    "\n",
    "- **Accuracy Score using TF-IDF:** 1.0\n",
    "- **F1 Score:** 1.0\n",
    "- **Cross-Validation Accuracy using TF-IDF:** 0.8\n",
    "\n",
    "#### Possible Reasons for the Results:\n",
    "- **Accuracy Score and F1 Score of 1.0:** Similar to the previous models, the perfect accuracy and F1 scores indicate that the **Logistic Regression** model performs exceptionally well on the test data. This could again point to **overfitting**, where the model fits the training data too well and fails to generalize effectively to new, unseen data.\n",
    "\n",
    "- **Cross-Validation Accuracy:** While the accuracy on the test data is perfect, the cross-validation scores are lower, with BOW scoring 0.9 and TF-IDF scoring 0.8. These lower scores suggest that while the model performs well in specific splits of the data, it might not generalize as reliably across all data subsets, again hinting at potential **overfitting**.\n",
    "\n",
    "Overall, the results highlight that while the **Logistic Regression** model might appear to perform well on the test set, there are signs of overfitting due to the discrepancy between the high test accuracy and the lower cross-validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de27e9d-69a3-4952-9d04-d2663696414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score using bow: 1.0\n",
      "F1 Score: 1.0\n",
      "Cross-Validation Accuracy using bow: 0.9\n",
      "\n",
      "Accuracy Score using tfidf: 1.0\n",
      "F1 Score: 1.0\n",
      "Cross-Validation Accuracy using tfidf: 0.8\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# Logistic Regression using bow\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train_bow, y_train)\n",
    "y_pred_lg=clf.predict(X_test_bow)\n",
    "print(\"Accuracy Score using bow:\",accuracy_score(y_test,y_pred_lg))\n",
    "f1=f1_score(y_test, y_pred_lg, average='weighted')\n",
    "print(\"F1 Score:\",f1)\n",
    "cv_scores = cross_val_score(clf, X_train_bow, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy using bow:\", cv_scores.mean())\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# Logistic Regression using tfidf\n",
    "clftf = LogisticRegression(random_state=42)\n",
    "clftf.fit(X_train_tfidf, y_train)\n",
    "y_pred_lgtf=clf.predict(X_test_tfidf)\n",
    "print(\"\\nAccuracy Score using tfidf:\",accuracy_score(y_test,y_pred_lgtf))\n",
    "f1=f1_score(y_test, y_pred_lgtf, average='weighted')\n",
    "print(\"F1 Score:\",f1)\n",
    "cv_scores = cross_val_score(clf, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy using tfidf:\", cv_scores.mean())\n",
    "#-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b707d2-3c9a-4afe-92b3-c36720748d7b",
   "metadata": {},
   "source": [
    "### 10. Model Evaluation Using Support Vector Machine (BOW)\n",
    "\n",
    "In this step, the **Support Vector Machine (SVM)** with a **linear kernel** is applied to the dataset using the **Bag of Words (BOW)** vectorization technique. The model is trained and evaluated on the test data (`X_test_bow`) using accuracy, F1 score, and cross-validation.\n",
    "\n",
    "The code trains the **SVM** model on the BOW representation of the training data and evaluates its performance.\n",
    "\n",
    "#### Results:\n",
    "- **Accuracy Score using BOW:** 1.0\n",
    "- **F1 Score:** 1.0\n",
    "- **Cross-Validation Accuracy using BOW:** 0.9\n",
    "\n",
    "#### Possible Reasons for the Results:\n",
    "- **Accuracy Score and F1 Score of 1.0:** The perfect accuracy and F1 scores suggest that the **SVM** model performs exceptionally well on the test data. As with the previous models, this could indicate **overfitting**, where the model fits the training data too well but may not generalize well to unseen data.\n",
    "\n",
    "- **Cross-Validation Accuracy:** The cross-validation accuracy of 0.9 is lower than the perfect accuracy on the test set, which again suggests that while the model performs well on specific test splits, it might struggle to generalize to different subsets of the data. This discrepancy points to **overfitting** as a potential issue.\n",
    "\n",
    "In summary, although the **SVM model** performs perfectly on the test data, the lower cross-validation accuracy suggests that it may be overfitting, highlighting the need for further model tuning to improve generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f63541f-b550-45da-a150-01691cb7c1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score using bow: 1.0\n",
      "F1 Score: 1.0\n",
      "Cross-Validation Accuracy using bow: 0.9\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# Support Vector Machine using bow\n",
    "svm=SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred_svm=svm.predict(X_test_bow)\n",
    "print(\"Accuracy Score using bow:\",accuracy_score(y_test,y_pred_svm))\n",
    "f1=f1_score(y_test, y_pred_svm, average='weighted')\n",
    "print(\"F1 Score:\",f1)\n",
    "cv_scores = cross_val_score(clf, X_train_bow, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy using bow:\", cv_scores.mean())\n",
    "#-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10dc95-7d94-4543-8b1a-3afef020fe32",
   "metadata": {},
   "source": [
    "### 11. Final Model - Voting Classifier\n",
    "\n",
    "In this step, a **Voting Classifier** is created using three different models: **GaussianNB**, **Logistic Regression**, and **Support Vector Machine (SVM)**. The models are combined using **hard voting**, where the final prediction is based on the majority vote from the individual models.\n",
    "\n",
    "The purpose of using the **Voting Classifier** is to combine the strengths of different models to create a more robust classifier. By doing so, the model may be better at generalizing patterns and distinguishing between **publishable** and **non-publishable** papers.\n",
    "\n",
    "#### Results:\n",
    "- **Accuracy Score:** 1.0\n",
    "- **Cross-Validation Accuracy using BOW:** 0.9\n",
    "- **F1 Score:** 1.0\n",
    "\n",
    "#### Observations and Conclusion:\n",
    "- **Accuracy and F1 Scores:** The Voting Classifier provides perfect accuracy and F1 scores on the test set (X_test) and also on the temporary validation set (X_temp). This indicates that the model is able to learn to correctly distinguish between publishable and non-publishable papers.\n",
    "  \n",
    "- **Cross-Validation Accuracy:** The cross-validation accuracy of 0.9 suggests that the model generalizes well to different data subsets, further supporting the idea that the model is not overfitting.\n",
    "\n",
    "- **Overfitting Assessment:** Despite the high accuracy and F1 scores, the model's performance on different data splits (via cross-validation) suggests that it is not overfitting. Moreover, the content and style of the papers (publishable vs non-publishable) exhibit clear differences that the machine learning models (GaussianNB, Logistic Regression, SVM) can easily capture, even with a smaller dataset. Given this distinction in content and style, the model likely learns to distinguish patterns effectively without overfitting.\n",
    "\n",
    "In conclusion, after reviewing the results and the data, it can be reasonably assumed that the model is not overfitting and has learned the correct patterns to differentiate between **publishable** and **non-publishable** papers. The high performance, combined with the use of a Voting Classifier, makes this approach more robust and reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c9d4a1-eda5-49c1-84ac-7cd5dce442e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 1.0\n",
      "Cross-Validation Accuracy using bow: 0.9\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('gnb', gnb),\n",
    "    ('lr', clf),\n",
    "    ('svm', svm)\n",
    "], voting='hard')\n",
    "voting_clf.fit(X_train_bow, y_train)\n",
    "y_pred_vc = voting_clf.predict(X_test_bow)\n",
    "print(\"Accuracy Score:\",accuracy_score(y_test,y_pred_vc))\n",
    "f1=f1_score(y_test, y_pred_vc, average='weighted')\n",
    "cv_scores = cross_val_score(voting_clf, X_train_bow, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy using bow:\", cv_scores.mean())\n",
    "print(\"F1 Score:\",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa25c4-a3b2-4ea4-92a1-ea5d4c37c8f0",
   "metadata": {},
   "source": [
    "### Checking Predictions for a Specific PDF\n",
    "\n",
    "This code is used to observe the predictions made by different models (GaussianNB, Logistic Regression, SVM, and Voting Classifier) for a given PDF document. The process involves:\n",
    "\n",
    "#### Purpose:\n",
    "This code helps to:\n",
    "- Examine how individual models and the ensemble Voting Classifier predict for a specific input.\n",
    "- Compare the performance and consistency of different models on the same document.\n",
    "\n",
    "#### Note:\n",
    "This step is optional and can be used solely to observe model predictions on a given PDF. If such comparisons are not required, this step can be ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24179f9b-ec27-47cb-abf8-74dc307f0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction due to Bag of Words(GaussianNB):  [1]\n",
      "\n",
      "\n",
      "Prediction due to Bag of Words(Logistic Regression):  [1]\n",
      "\n",
      "\n",
      "Prediction due to Bag of Words(SVM):  [1]\n",
      "\n",
      "\n",
      "Prediction due to Bag of Words(Voting Classifier):  [1]\n"
     ]
    }
   ],
   "source": [
    "text=text_extractor_from_pdf('Samples/Sample10.pdf')\n",
    "temp=preprocessed_text(text)\n",
    "\n",
    "temp_bow=cv.transform([temp]).toarray()\n",
    "temp_tfidf=tfidf.transform([temp]).toarray()\n",
    "\n",
    "y_gn_bow=gnb.predict(temp_bow)\n",
    "y_gn_tf=gnbtf.predict(temp_tfidf)\n",
    "\n",
    "y_lf_bow=clf.predict(temp_bow)\n",
    "y_lf_tf=clftf.predict(temp_tfidf)\n",
    "\n",
    "y_svm_bow=svm.predict(temp_bow)\n",
    "\n",
    "y_voting_clf_bow=voting_clf.predict(temp_bow)\n",
    "print('Prediction due to Bag of Words(GaussianNB): ',y_gn_bow)\n",
    "# print('Prediction due to Tf-Idf(GaussianNB): ',y_gn_tf)\n",
    "print('\\n')\n",
    "print('Prediction due to Bag of Words(Logistic Regression): ',y_lf_bow)\n",
    "# print('Prediction due to Tf-Idf(Logistic Regression): ',y_lf_tf)\n",
    "print('\\n')\n",
    "print('Prediction due to Bag of Words(SVM): ',y_svm_bow)\n",
    "print('\\n')\n",
    "print('Prediction due to Bag of Words(Voting Classifier): ',y_voting_clf_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70b130-b8a8-4c45-8a6a-b1ef94735aa6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4aa1f-4869-49a5-87c0-7ea8c2c72c87",
   "metadata": {},
   "source": [
    "### `Why Not Use Deep Learning Models?`\n",
    "\n",
    "We didn’t use any deep learning-based models to classify the papers because the dataset is very small. Deep learning models require a large amount of data to train effectively and generalize well. With such a small dataset, it’s well-known that deep learning models would fail to classify the papers correctly. That’s why we chose simpler machine learning models like GaussianNB, Logistic Regression, and SVM, which perform much better for smaller datasets like this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32dda1-3176-42d9-b4ac-5e9fe18e3d7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068880b-e5f1-45e2-b217-0f15472ce660",
   "metadata": {},
   "source": [
    "###                                                    `Task 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6597c92-68a4-47b9-8559-4dc2d918f299",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up Pathway for Google Drive Integration\n",
    "\n",
    "This code sets up Pathway to monitor and process files in a Google Drive folder, serving as the foundation for our project. Here's the breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "#### Process Overview\n",
    "\n",
    "1. **Google Drive Integration**:\n",
    "   - We use the `pw.io.gdrive.read` function to connect Pathway to a specific Google Drive folder.\n",
    "   - Parameters explained:\n",
    "     - `object_id`: The unique ID of the Google Drive folder (explained below).\n",
    "     - `service_user_credentials_file`: The JSON credentials file for the service account that provides secure access to the folder.\n",
    "     - `mode=\"streaming\"`: Ensures real-time monitoring of the folder for new or updated files.\n",
    "     - `with_metadata`: Captures additional file details like names and timestamps.\n",
    "     - `refresh_interval=10`: Sets a refresh interval of 10 seconds for checking folder updates.\n",
    "\n",
    "2. **Creating the `object_id`**:\n",
    "   - We created a **Google Drive folder** specifically for this project.\n",
    "   - A **service account** was set up, with the associated JSON credentials file downloaded.\n",
    "   - The service account was granted access to this folder.\n",
    "   - The `object_id` is the unique folder identifier, extracted from the folder's URL.\n",
    "   - Folder URL: https://drive.google.com/drive/folders/1POboCZVq6bzgVaL-b_7milEq9FEEff2p?usp=sharing\n",
    "\n",
    "3. **Setting Up Data Sources**:\n",
    "   - The `data_sources` list includes a table created using `pw.io.gdrive.read`. This table continuously streams and processes the folder's contents.\n",
    "\n",
    "4. **Running Pathway in a Thread**:\n",
    "   - Pathway is executed in a separate thread using `threading.Thread` and the `pw.run()` function. This ensures asynchronous real-time processing without interrupting other operations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Step Is Important\n",
    "- **Real-Time Processing**: The folder is monitored for changes, ensuring that newly uploaded files are promptly processed.\n",
    "- **Secure Access**: By using a service account, we ensure controlled and secure interaction with the Google Drive folder.\n",
    "- **Asynchronous Execution**: Running Pathway in a separate thread makes it possible to handle other tasks simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "#### Practical Application\n",
    "This setup allows us to upload research files to a dedicated Google Drive folder, which Pathway can monitor and process in real-time. This approach is particularly useful for managing evolving datasets or shared resources.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26b87d1b-719b-4285-a8a5-e54f89842f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1049h\u001b[H\u001b[?25l\u001b[H                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                      LOGS                                      \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \u001b[?25h\u001b[?1049l"
     ]
    }
   ],
   "source": [
    "import pathway as pw\n",
    "import threading\n",
    "\n",
    "# Function to run Pathway in a separate thread\n",
    "def run_pathway():\n",
    "    pw.run()\n",
    "\n",
    "# Add the data to Pathway Vectorstore\n",
    "table = pw.io.gdrive.read(\n",
    "    object_id=\"1POboCZVq6bzgVaL-b_7milEq9FEEff2p\",\n",
    "    service_user_credentials_file=\"credentials.json\",\n",
    "    mode=\"streaming\",\n",
    "    with_metadata=True,\n",
    "    refresh_interval=10,\n",
    ")\n",
    "\n",
    "# Add processed documents from Google Drive\n",
    "data_sources = [table]\n",
    "# data_sources.append(table)\n",
    "\n",
    "# Start Pathway in a separate thread\n",
    "pathway_thread = threading.Thread(target=run_pathway, daemon=True)\n",
    "pathway_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6f88b-42a9-4387-8e2d-ca8a57c4d142",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up the Pathway VectorStore Server for Embedding and Querying\n",
    "\n",
    "This step focuses on configuring the **Pathway VectorStore Server** to process and embed data from the monitored Google Drive folder. Here's how it works:\n",
    "\n",
    "---\n",
    "\n",
    "#### Process Overview\n",
    "\n",
    "1. **Importing Required Modules**:\n",
    "   - Pathway's `llm` extension is used to handle embedding, splitting, parsing, and vector storage functionalities.\n",
    "\n",
    "2. **Defining Components**:\n",
    "   - **`TokenCountSplitter`**: Splits the text into smaller chunks based on token count. This is useful for handling large documents without exceeding token limits.\n",
    "   - **`GeminiEmbedder`**: Uses the Gemini API to generate embeddings for text data. This API requires an `api_key` for secure access.\n",
    "   - **`ParseUnstructured`**: Parses raw text using the `preprocessed_text` function, which ensures that the input is cleaned and standardized before embedding.\n",
    "\n",
    "3. **Initializing the VectorStore Server**:\n",
    "   - **Data Sources**: Includes all the files monitored by Pathway in the Google Drive folder (from Step 1).\n",
    "   - **Parser**: Processes raw input files into structured, preprocessed text.\n",
    "   - **Embedder**: Generates numerical embeddings for the text, which are stored in the vector database for similarity-based operations.\n",
    "   - **Splitter**: Divides the text into manageable chunks, enhancing the embedding and retrieval processes.\n",
    "\n",
    "4. **Running the VectorStore Server**:\n",
    "   - **Configuration**: The server is configured to run locally (`127.0.0.1`) on a specified port (`8765`).\n",
    "   - **Threaded Mode**: Ensures non-blocking execution, allowing multiple processes to interact with the server simultaneously.\n",
    "   - **Cache Disabled**: For this instance, caching is turned off to avoid stale data issues during processing.\n",
    "\n",
    "5. **Workaround for Colab**:\n",
    "   - A `time.sleep(30)` command is added to keep the cell running for 30 seconds, as threads in Colab require active cells for visibility. \n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Step Is Important\n",
    "- **Efficient Data Handling**: The use of text splitting and preprocessing ensures smooth handling of large or unstructured files.\n",
    "- **Embedding for Similarity Queries**: By embedding text into numerical vectors, the server allows semantic searches and similarity-based operations.\n",
    "- **Real-Time Vector Database**: Enables the system to store and retrieve vector embeddings in real time, crucial for applications like search engines, recommendation systems, and classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### Practical Application\n",
    "This setup enables us to process and store embeddings for research papers or other documents uploaded to Google Drive. The embeddings can later be used for querying similar documents, clustering, or other machine learning tasks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4dc39f0-2713-46e7-8fcb-9a1af688808f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running on http://127.0.0.1:8765 ========\n",
      "(Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from pathway.xpacks.llm.embedders import GeminiEmbedder\n",
    "from pathway.xpacks.llm.splitters import TokenCountSplitter\n",
    "from pathway.xpacks.llm.vector_store import VectorStoreClient, VectorStoreServer\n",
    "from pathway.xpacks.llm.parsers import ParseUnstructured\n",
    "PATHWAY_PORT = 8765\n",
    "\n",
    "text_splitter = TokenCountSplitter()\n",
    "embedder = GeminiEmbedder(api_key=GEMINI_API_KEY)\n",
    "parser=ParseUnstructured(\n",
    "    mode='single',\n",
    "    post_processors=[preprocessed_text]\n",
    ")\n",
    "\n",
    "vector_server = VectorStoreServer(\n",
    "    *data_sources,\n",
    "    parser=parser,\n",
    "    embedder=embedder,\n",
    "    splitter=text_splitter,\n",
    ")\n",
    "vector_server.run_server(host=\"127.0.0.1\", port=PATHWAY_PORT, threaded=True, with_cache=False)\n",
    "time.sleep(30)  # Workaround for Colab - messages from threads are not visible unless a cell is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679660a7-ce79-46a9-8050-2dc05aa008ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table.schema.column_names()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710bc74-db5d-4471-9390-5f6517c3a4fe",
   "metadata": {},
   "source": [
    "### Step 3: Connecting to the VectorStore Server\n",
    "\n",
    "In this step, we establish a connection to the **Pathway VectorStore Server** set up in Step 2.\n",
    "\n",
    "---\n",
    "\n",
    "#### Process Overview\n",
    "\n",
    "1. **`VectorStoreClient` Initialization**:\n",
    "   - The `VectorStoreClient` connects to the running **VectorStore Server** instance. \n",
    "   - The connection is established by specifying:\n",
    "     - **Host**: `127.0.0.1` (local server).\n",
    "     - **Port**: The port used by the server, defined as `PATHWAY_PORT` (set to 8765 in Step 2).\n",
    "\n",
    "2. **Purpose of the Client**:\n",
    "   - This client acts as an interface to communicate with the server, enabling operations like querying, adding new data, or retrieving embeddings from the vector database.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Step Is Important\n",
    "- **Server Communication**: The client is essential for interacting with the vector database created by the server.\n",
    "- **Data Retrieval and Querying**: Using this connection, we can perform semantic searches, retrieve similar documents, or integrate embeddings into downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### Practical Application\n",
    "With this client, we can:\n",
    "- Query the vector database to find similar documents based on embeddings.\n",
    "- Use the embeddings stored in the database for tasks like classification, clustering, or recommendation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eb856ea-562a-4705-8d5a-0ac94bd03466",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = VectorStoreClient(\n",
    "    host=\"127.0.0.1\",\n",
    "    port=PATHWAY_PORT,\n",
    "    timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2c5b0-a767-4255-a8f8-4fea3ae7f767",
   "metadata": {},
   "source": [
    "### Step 4: Loading Conference Papers into Vector Store and Running the Server\n",
    "\n",
    "In this step, we load the conference papers into a vector store and run a server to handle vector-based queries. Here's how the process works:\n",
    "\n",
    "1. **Conference Folders Setup**:\n",
    "   - A dictionary `conference_folders` is defined, mapping each conference name to its respective folder path. Each folder contains PDF papers related to a specific conference (e.g., CVPR, EMNLP, KDD, NeurIPS, TMLR).\n",
    "\n",
    "2. **Reading PDF Files**:\n",
    "   - The code iterates through the conference folder paths and uses the `pw.io.fs.read` function to read the PDF files.\n",
    "   - The files are read using a binary format, ensuring that metadata is included, and the files are ingested in static mode, meaning they are processed once and not repeatedly loaded.\n",
    "   - The resulting table for each conference is appended to the `reference_sources` list.\n",
    "\n",
    "3. **Creating Vector Store Server**:\n",
    "   - The `VectorStoreServer` is initialized with the loaded conference papers and necessary configurations such as a parser, embedder, and text splitter.\n",
    "   - The server is then run on `127.0.0.1` at port `8000`, enabling vector-based queries for the conference papers.\n",
    "\n",
    "4. **Running the Server**:\n",
    "   - The server is started in a threaded mode, allowing for concurrent handling of multiple requests.\n",
    "\n",
    "This setup ensures that the conference papers are ready to be queried based on vector similarities, enabling efficient classification and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7042cf9-4f90-4589-91e6-982bed314318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Thread(VectorStoreServer, started 14465888256)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conference_folders = {\n",
    "    \"CVPR\": r\"Reference/Publishable/CVPR\",\n",
    "    \"EMNLP\": r\"Reference/Publishable/EMNLP\",\n",
    "    \"KDD\": r\"Reference/Publishable/KDD\",\n",
    "    \"NeurIPS\": r\"Reference/Publishable/NeurIPS\",\n",
    "    \"TMLR\": r\"Reference/Publishable/TMLR\"\n",
    "}\n",
    "\n",
    "reference_sources=[]\n",
    "for conference_name, folder_path in conference_folders.items():\n",
    "    table = pw.io.fs.read(\n",
    "        path=folder_path + \"/*.pdf\",  # Glob pattern to match all PDF files\n",
    "        format=\"binary\",\n",
    "        with_metadata=True,\n",
    "        mode=\"static\",  # Static mode to ingest data once\n",
    "    )\n",
    "\n",
    "    reference_sources.append(table)\n",
    "\n",
    "# Create a vector store server for conferences\n",
    "vector_server = VectorStoreServer(\n",
    "    *reference_sources,\n",
    "    parser=parser,\n",
    "    embedder=embedder,\n",
    "    splitter=text_splitter,\n",
    ")\n",
    "vector_server.run_server(host=\"127.0.0.1\", port=8000, threaded=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee034c8d-fbfd-4844-9432-416c8212a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3031e58d-c96e-4c76-ab5c-68d817f9f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tv in reference_sources:\n",
    "#     print(tv.schema.column_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc604082-80e6-4871-9fd8-32e717510267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_table=[]\n",
    "# for table in reference_sources:\n",
    "#     content_table.append(table.select(pdf_content=table.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dfdf338-a7db-4b0b-b705-991332e0d828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # pw.debug.compute_and_print(content_table[0], include_id=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab93872-dbf1-43e7-84b8-67df6961b353",
   "metadata": {},
   "source": [
    "### Step 5: Paper Classification using Gemini and VectorStore\n",
    "\n",
    "In this step, we implement a system to classify research papers into conferences based on their content using the **Gemini** model and a preloaded vector store. Here's how the process works:\n",
    "\n",
    "1. **PDF Extraction**:\n",
    "   - The `PDFReader` class is responsible for reading PDF files from specified conference folders.\n",
    "   - It scans all files in the provided folder, extracts the content from each PDF, and stores the extracted text in a dictionary.\n",
    "   - The `__call__` method is used to trigger the extraction process, reading the text from each page in the PDF file.\n",
    "\n",
    "2. **Vector Store Setup**:\n",
    "   - We configure a `VectorStoreClient` to connect to a local vector store running on `127.0.0.1:8000`.\n",
    "   - The papers from each conference folder are processed and added to the vector store, where they will be stored as embeddings for future similarity-based searches.\n",
    "\n",
    "3. **Paper Classification**:\n",
    "   - When a new paper is provided for classification, its content is extracted using the `PDFReader` class.\n",
    "   - The extracted text is sent to the vector store to perform a similarity search. The most relevant paper from the preloaded papers is identified based on the similarity score.\n",
    "   - The system uses the **Gemini** model to classify the paper. A prompt is constructed with the paper content and relevant passage from the search, asking Gemini to classify the paper into one of the predefined conferences.\n",
    "\n",
    "4. **Using Gemini API**:\n",
    "   - The `genai` library is used to interact with the Gemini model. It is configured with an API key (`GEMINI_API_KEY`), which is required for the model to generate the response.\n",
    "   - The generated response contains the classification (i.e., the most relevant conference) and a rationale explaining why the paper belongs to that particular conference.\n",
    "\n",
    "5. **Final Output**:\n",
    "   - The system returns the classification and rationale for the paper, providing a clear understanding of why the paper was assigned to the specific conference.\n",
    "\n",
    "---\n",
    "\n",
    "### Important Notes:\n",
    "- Replace the placeholder `GEMINI_API_KEY` with your actual Gemini API key to ensure proper functionality.\n",
    "- The vector store should be running and accessible at `127.0.0.1:8000` for successful integration with the classification system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75e761c-25a1-4e41-8998-6d63babc520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathway.stdlib.indexing import BruteForceKnnFactory, HybridIndexFactory\n",
    "# from pathway.stdlib.indexing.bm25 import TantivyBM25Factory\n",
    "\n",
    "# hybrid_index = HybridIndexFactory(\n",
    "#     [\n",
    "#         TantivyBM25Factory(),  # BM25-based keyword search\n",
    "#         BruteForceKnnFactory(embedder=embedder)  # Vector-based semantic search\n",
    "#     ]\n",
    "# )\n",
    "# for docs in publishable_texts:\n",
    "#     hybrid_index.build_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa1c0323-cefb-4a87-817a-81decb52606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(hybrid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44c3fb25-9f7d-4a12-bd43-e0d41d03cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "\n",
    "resource_client = VectorStoreClient(\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8000,\n",
    "    timeout=120\n",
    ")\n",
    "\n",
    "def classify_paper(paper_text,conferences, genai_model):\n",
    "    # Perform similarity search\n",
    "    result = resource_client.query(query=[paper_text], k=1)\n",
    "    \n",
    "    if not result:\n",
    "        return \"Could not classify the paper.\", \"\"\n",
    "\n",
    "    closest_match = result[0]\n",
    "    metadata = closest_match[\"metadata\"]\n",
    "    # classification = metadata[\"conference\"]\n",
    "    # # Extract relevant passage\n",
    "    relevant_passages = closest_match[\"text\"]\n",
    "    passage_oneline = \" \".join(relevant_passages).replace(\"\\n\", \" \")\n",
    "\n",
    "    # Generate classification and rationale\n",
    "    prompt = (\n",
    "        f\"Classify the following paper into one of these conferences: {', '.join(conferences)}.\\n\"\n",
    "        f\"Paper Content (trimmed): {paper_text}...\\n\"\n",
    "        f\"Relevant Passage: {passage_oneline}\\n\"\n",
    "        f\"Provide the closest match classification from the listed conferences. The classification must not be 'None of the above'\\n\"\n",
    "        f\"Additionally, provide a separate rationale for the classification (not more than 100 words). Start your rationale with 'Rationale:'.\"\n",
    ")\n",
    "\n",
    "\n",
    "    response = genai_model.generate_content(prompt).parts[0].text\n",
    "\n",
    "    # Split response into classification and rationale\n",
    "    if \"Rationale:\" in response:\n",
    "        classification, rationale = response.split(\"Rationale:\", maxsplit=1)\n",
    "        classification = classification.replace(\"**Classification:**\", \"\").replace(\"**\",\"\").strip()\n",
    "        rationale = rationale.replace(\"**\", \"\").strip()\n",
    "    else:\n",
    "        classification = response.strip()\n",
    "        rationale = \"Rationale not provided.\"\n",
    "\n",
    "    return classification, rationale\n",
    "\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "genai_model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "def classify_rationale(paper_path):\n",
    "    if not os.path.exists(paper_path) or not paper_path.endswith(\".pdf\"):\n",
    "        print(\"Invalid file. Please try again.\")\n",
    "    \n",
    "    with open(paper_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        paper_text = \" \".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        classification, rationale = classify_paper(paper_text, list(conference_folders.keys()), genai_model)\n",
    "    return classification,rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6a33198-089e-4415-a845-a9d9f562b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource_client.get_vectorstore_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8472077-1791-42b0-a3f8-b40490555066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pathway_engine.persistence.state:Rotation id is unparsable from the key 2-0-0 2\n",
      "ERROR:pathway_engine.persistence.state:Rotation id is unparsable from the key 2-0-0 2\n",
      "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-4: Closing the data source\n",
      "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-5: Closing the data source\n",
      "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-6: Closing the data source\n",
      "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-7: Closing the data source\n",
      "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-8: Closing the data source\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running on http://127.0.0.1:8000 ========\n",
      "(Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file_count': 1, 'last_modified': None, 'last_indexed': 1737186575}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_vectorstore_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3197e-a512-4ac5-bc0a-b0b8b5ca56b3",
   "metadata": {},
   "source": [
    "### Step 6: Managing the Results List\n",
    "\n",
    "In this part of the code, we initialize an empty DataFrame (`df2`) to store the results later. We define a folder path (`./GetDocuments`) where the downloaded PDF files will be stored. The `os.makedirs()` function ensures that this folder is created if it doesn't already exist. \n",
    "\n",
    "We also define the `prefix` variable, which holds the base URL needed to construct the download link for each file from Google Drive. The empty list `results` will store the classification information of the papers and is used globally across the process. \n",
    "\n",
    "#### Purpose:\n",
    "- **Create necessary directories**: The `./GetDocuments` folder is created to store the PDFs downloaded from Google Drive.\n",
    "- **Prepare the results container**: The `results` list is initialized here so that it can be used across multiple code snippets to accumulate the classification and metadata of the papers.\n",
    "- **Define download URL prefix**: The `prefix` helps in forming the complete URL for downloading files from Google Drive.\n",
    "\n",
    "This is a setup phase, and it does not perform any processing yet, but ensures that we are ready to handle the download and classification process in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ce770d0-ef06-47b6-a0e9-a54d7f069dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown,sys\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "prefix='https://drive.google.com/uc?/export=download&id='\n",
    "os.makedirs('./GetDocuments', exist_ok=True)\n",
    "results=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4295d-98f3-452f-94ce-f4b27ecb57d3",
   "metadata": {},
   "source": [
    "### Step 7: Downloading and Classifying New Files\n",
    "\n",
    "In this part of the code, we retrieve the list of files (PDFs) from the vector store using the `client.get_input_files()` method. If there are any documents in the vector store, the code proceeds to download the PDFs from Google Drive`( https://drive.google.com/drive/folders/1POboCZVq6bzgVaL-b_7milEq9FEEff2p?usp=sharing)` using the `gdown.download()` method.\n",
    "\n",
    "For each document:\n",
    "1. **File Download**: The PDF files are downloaded to the `./GetDocuments` folder using the file URL and `gdown`.\n",
    "2. **Text Extraction and Preprocessing**: The content of each downloaded PDF is extracted and preprocessed with the `preprocessed_text()` function.\n",
    "3. **Bag of Words Transformation**: The preprocessed text is transformed using the BOW (Bag of Words) method.\n",
    "4. **Prediction**: The model predicts whether the paper is publishable based on the BOW transformation, and the classification and rationale are determined using the preprocessed content. \n",
    "5. **Result Collection**: All results (publishable status, classification, rationale) are added to the `results` list, which is then used to generate a DataFrame (`df2`).\n",
    "\n",
    "#### Purpose:\n",
    "- **Download PDFs**: PDFs stored in the vector store are downloaded to the local machine, ensuring we work with the latest documents.\n",
    "- **Text Extraction and Transformation**: We extract and preprocess the text from the PDFs, which is crucial for accurate prediction and classification.\n",
    "- **Prediction**: The model (based on BOW) is used to determine whether a paper is publishable, followed by a classification using the LLM model, providing a rationale for the classification.\n",
    "- **Storing Results**: The results of each file's classification are stored in the `results` list, which is then formatted into a DataFrame (`df2`) for easier access.\n",
    "\n",
    "`This code needs to be manually run after the new files are stored in the vector store.It will not run automatically after files are added to the vector store. The user should manually execute this code to classify the newly uploaded documents and generate the results DataFrame (`df2`).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a383c572-a241-4652-bb9f-839fc5e9e50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading R008.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?/export=download&id=1uJpRdAHtRmohbfX0UP1lWdagjxPzHiT3\n",
      "To: /Users/malyadippal/Desktop/Code_Conquerors_KDSH_ROUND2/Task/GetDocuments/R008.pdf\n",
      "100%|██████████████████████████████████████| 70.9k/70.9k [00:00<00:00, 1.00MB/s]\n"
     ]
    }
   ],
   "source": [
    "document_info = client.get_input_files()\n",
    "if(len(document_info)):\n",
    "    for doc in document_info:\n",
    "        file_name=doc['name']\n",
    "        file_id=doc['url'].split('/')[-2]\n",
    "        output_path = f'./GetDocuments/{file_name}'  # Set the desired output path\n",
    "        if not os.path.exists(output_path):\n",
    "            print(f\"Downloading {file_name}...\")\n",
    "            gdown.download(prefix + file_id, output_path, quiet=False)\n",
    "        else:\n",
    "            print(f\"{file_name} already exists. Skipping download.\")\n",
    "            continue\n",
    "        processed_text=preprocessed_text(text_extractor_from_pdf(output_path))\n",
    "        processed_text_bow=cv.transform([processed_text]).toarray()\n",
    "        pt_voting_clf_bow=voting_clf.predict(processed_text_bow)\n",
    "        if (pt_voting_clf_bow[0]):\n",
    "            classification,rationale=classify_rationale(output_path)\n",
    "            time.sleep(2)\n",
    "            results.append((file_name.split('.')[0],pt_voting_clf_bow[0],classification,rationale))\n",
    "        else:\n",
    "            results.append((file_name.split('.')[0],pt_voting_clf_bow[0],'na','na'))\n",
    "        \n",
    "    df2 = pd.DataFrame(results, columns=['Paper ID', 'Publishable', 'Conference', 'Rationale'])\n",
    "    df2['Paper ID Number'] = df2['Paper ID'].str.extract(r'(\\d+)').astype(int)\n",
    "    df2 = df2.sort_values(by='Paper ID Number').drop(columns=['Paper ID Number'])\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "else:\n",
    "    print('No files present in drive!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a16b9018-0a40-46a8-8bcd-626df144101b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper ID</th>\n",
       "      <th>Publishable</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R008</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>The paper focuses on Noun-Noun compound interp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Paper ID  Publishable Conference  \\\n",
       "0     R008            1      EMNLP   \n",
       "\n",
       "                                           Rationale  \n",
       "0  The paper focuses on Noun-Noun compound interp...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "952e885c-075e-484e-84a0-905ce7f0cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96b3d546-b79a-4404-83e5-cfbf03ca0926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: EMNLP\n",
      "\n",
      "\n",
      "Rationale: The paper focuses on Chain-of-Thought (CoT) prompting in large language models, a topic central to natural language processing. It analyzes CoT's impact on reasoning tasks, sample complexity, and generalization, all within the scope of NLP and thus aligning with EMNLP's focus.  The mention of related work in question answering and commonsense reasoning further strengthens this classification.\n"
     ]
    }
   ],
   "source": [
    "classification,rationale=classify_rationale(\"papers/P008.pdf\")\n",
    "print(\"Classification:\",classification)\n",
    "print(\"\\n\")\n",
    "print(\"Rationale:\",rationale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
